---
title: "Notes on Time for a Change"
author: "Swirl Coders"
date: "11/25/2014"
output: html_document
---

```{r}
tfc <- if(interactive()){
  read.table("Topics_in_Generalized_Linear_Models/Time_for_a_Change_Election_Model/tfc.csv", as.is=TRUE, header=TRUE, sep=",")
  } else {
    read.table("tfc.csv", as.is=TRUE, header=TRUE, sep=",")
  }
```

### Background

In US presidential elections voters choose electors and electors choose the winner. In practice, electors are committed to candidates, but because electors are not strictly proportional to population, the winner of the popular vote may not win the election. This happened most recently in the 2000 election when Al Gore won the popular vote and George W. Bush won the Presidency.

Interest in the outcome of a presidential election begins long before the vote. Polls are taken and pundits speculate. Statisticians had always been in the background, but in 2008 statistics came to the fore with Nate Silver and his website [fivethirtyeight.com](http://fivethirtyeight.com), (so named because there are 538 electors.) In 2012, statistician Drew Linzer forecast election results precisely, using publically available data, R, and a model which he explains on his website [votamatic.org](http://votamatic.org/how-it-works/). Visit?

Drew Linzer's algorithm begins, about six months before the vote, with the Time-for-Change (TFC) model proposed by Alan Abromowitz. As the election approaches, Linzer weights polls more heavily and the Time-for-Change model less. However, we'll consider only the TFC model here. In its simplest form, it is a linear regression predicting the incumbent party's share of the popular vote in terms of an intercept, the incumbent president's net approval rating in June, the rate of economic growth in the second quarter, and a dummy variable which is 1 if the incumbent party has held the Presidency for two or more terms.

## The Time for Change Model

$pv \sim 51.3 + 0.1 \times approval + 0.6 \times q2 - 4.1 \times tfc$

where 

$pv$ is the incumbent party's percent of the popular vote

$approval$  is the incumbent President's net approval rating,

$q2$  is the rate of economic growth in the second quarter, and

$tfc$  is 1 if the incumbent party has held the White House for two or more terms.

As can be seen, the dummy variable has a large, negative coefficient, hence the model's name.

The TFC model does fit the available data fairly well, as the plot shows. The available data, however, cover only the 17 elections since both presidential approval ratings and quarterly economic performance data have been available.

```{r echo=FALSE, fig.show='hold'}
fit <- 51.3 + 0.1*tfc$approval + 0.6*tfc$q2 - 4.1*tfc$tfc
plot(tfc$year, tfc$pv, col=1, pch=19, xlab="Year", ylab="Incumbent Party's Percent", main="The Time for Change Model")
points(tfc$year, fit, col=2, pch=17)
legend('topright', c("Actual Vote", "TFC Estimate"), col=1:2, pch=c(19,17))
```

Data for the TFC model was loaded into your workspace at the beginning of this lesson. It is in a data frame named tfc. In addition to the variables mentioned above, it contains the year, an indicator of whether the incumbent party's candidate won, and the name of the winning candidate. Inspect the data now by typing tfc, head(tfc), or View(tfc).

```{r}
tfc
```

How trustworthy is a 4 parameter model based on 17 observations? Perhaps it is overfitted. Perhaps other plausible models would do just as well. Our objective in this lesson is to compare the TFC model with two generalized linear alternatives.

To begin, derive the TFC model itself by applying R's lm function the tfc data to predict pv in terms of an intercept, approval, q2, and the tfc dummy variable. Store the model in a variable called fit_tfc.

```{r}
fit_tfc <- lm(pv ~ approval + q2 + tfc, tfc)
```

Inspect the model using summary(fit_tfc)

According to the summary, all four coefficients are significantly different from zero. The term, "significantly," suggests there some uncertainty about their values. Where does this presumed uncertainty originate?
* With the data.
* With numerical imprecision.
* With the inventor of the TFC model.

The data is presumed to be generated by a process which involves randomness. In that sense the data is a random variable, (or a sequence of random variables if you prefer.) The coefficients of fit_tfc are the result of a deterministic procedure, linear regression, applied to a random variable. Does this mean the coefficients themselves are random variables?
* Yes
* No
* Maybe

The result of applying a deterministic procedure to a random variable is often called a statistic. The distribution which a statistic is assumed to follow will depend on assumptions about the data from which the statistic is derived. What embodies those assumptions?
* The model.
* The Central Limit Theorem.
* Linear regression.
* The data.

The assumptions embodied in the TFC model are that the continuous variables, pv, approval, and q2, are jointly normally distributed with means depending on the dummy variable, tfc. The parameters derived by linear regression maximize the likelihood of the observed data, given those assumptions. The R function, logLik, calculates the natural logarithm of this likelihood. Type logLik(fit_tfc) to calculate it now.

If the coefficients are statistics, so are other parameters of fit_tfc, such as standard errors. The significance of a coefficient is based on its distance from zero, measured in multiples of standard errors. Thus, a coefficient's significance is also based on the assumption that the data is generated in the way the model assumes. A different model, hence different assumptions, can change that. To illustrate, create a model called fit_win which predicts win in terms of an intercept, approval, and tfc. Use glm with family=binomial.

```{r}
fit_win <- glm(win ~ approval + q2 + tfc, tfc, family=binomial)
```


In this case, glm maximizes the likelihood of observing the data, assuming that approval, q2, and the log odds of a win are jointly normally distributed with means depending on the dummy variable. Note that these log odds are latent (not observed) but determine the probability of a win in any given election. Use summary(fit_win) to examine the parameters of this maximum likelihood model.

In this case, none of the coefficients are significant; all are well within two standard errors of zero. But if the coefficients were all zero, an hypothesis which we can't reject, then wins would not depend on the predictors at all. Use logLik to find the log likelihood of the data given this model.

The data is more likely under this model than under the original tfc model, (since -4.1 is bigger than -32.9,) despite the possible insignificance of its coefficients. Of course the likelihoods themselves are statistics. They, too, depend on the data and the assumptions implicit in the model. In effect, they are probabilities conditional on model assumptions and computed parameters.

Although it is natural to use the binary win/loss outcome with a binomial glm model, it is not the same thing as using percent of the popular vote, pv. In that sense, fit_tfc and fit_win are not models of precisely the same data. However, we could have used probabilities rather than binary outcomes in the model. To do so we would merely divide pv by 100 to convert from percent to fraction of popular vote, hence be using essentially the same data. 

Create a model named fit_pv using glm(pv/100 ~ approval + q2 + tfc, tfc, family=binomial). The command will result in a warning that non-integer, not binary, outcomes are being used. Since we have used probabilities on purpose, the warning may be safely ignored. Don't forget to divide pv by 100.

```{r fig.show='hold'}
fit_pv <- glm(pv/100 ~ approval + q2 + tfc, tfc, family=binomial)
plot(tfc$year, tfc$pv, col=1, pch=19, xlab="Year", ylab="Incumbent Party's Percent", main="Binomial GLM Model with Probabilities\nrather than Binary Outcomes")
points(tfc$year, fit_pv$fitted.values*100, col=2, pch=17)
legend('topright', c("Actual Vote", "Model Estimate"), col=1:2, pch=c(19,17))
```

In this case the continuous predictors and the log odds, log(p/(1-p)), of the given probabilities, p = pv/100, are assumed to be jointly normally distributed with means depending on the dummy variable, tfc. In effect, p represents the probability that a random voter will vote for the candidate of the incumbent party. As can be seen in the figure, appears to fit the data as well as the original TFC model. Use logLik to calculate the data's likelihood given this model. 

The data is also more likely under this model than under the original TFC model, (since -10.4 > -32.9.) Use summary(fit_pv) to see whether this model's coefficients are significant.

We can't reject the hypothesis that the model's coefficients are zero. In fact, the coefficient of approval is well within one standard error of zero and could be dropped without serious effect.

The point has been made, so stop here and ask review questions. Points to review:
* The model embodies assumptions about the dgp.
* The data is a random variable.
* The outcome of a computation, such as lm or glm, on the data is also a random variable.
* The presumed distributions of model parameters depend on assumptions about the distribution of the data, hence on the assumptions embodied in the model.
* Given the assumptions of the TFC model the regressors are significant predictors of the election outcome. Given the assumptions of two other plausible models, they are not.    

### Reference
> http://votamatic.org/how-it-works

